from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import time
from bs4 import BeautifulSoup
import json
import os

def crawl_list(query, scroll_times=3, wait_time=2):
    driver = webdriver.Chrome()
    driver.get("https://www.google.com/maps")
    time.sleep(4)

    search_box = driver.find_element(By.ID, "searchboxinput")
    search_box.send_keys(query)
    search_box.send_keys(Keys.ENTER)
    time.sleep(6)  # tÄƒng thá»i gian load láº§n Ä‘áº§u

    # Cuá»™n Ä‘á»ƒ load thÃªm káº¿t quáº£
    for _ in range(scroll_times):
        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)
        time.sleep(wait_time)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    results = soup.find_all('div', class_='Nv2PK')
    print(f"ğŸ” TÃ¬m tháº¥y {len(results)} káº¿t quáº£ cho: {query}")

    data = []
    for r in results:
        try:
            link_tag = r.find('a', class_='hfpxzc')
            if link_tag:
                raw_name = link_tag.get('aria-label', '').strip()
                name = raw_name.split('Â·')[0].strip() if raw_name else 'N/A'
                link = link_tag.get('href', 'N/A')
            else:
                name = 'N/A'
                link = 'N/A'

            rating_tag = r.find('span', class_='MW4etd')
            rating = rating_tag.get_text(strip=True) if rating_tag else 'N/A'

            address_tag = r.find('div', class_='rllt__details')
            address = address_tag.get_text(strip=True) if address_tag else 'N/A'

            item = {
                "name": name,
                "address": address,
                "rating": rating,
                "link": link
            }
            data.append(item)
        except Exception as e:
            print(f"âŒ Lá»—i khi parse 1 káº¿t quáº£: {e}")

    driver.quit()
    return data

# def crawl_detail(link):
    driver = webdriver.Chrome()
    driver.get(link)
    time.sleep(6)  # tÄƒng thá»i gian load chi tiáº¿t

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # Láº¥y Ä‘á»‹a chá»‰ chi tiáº¿t
    address = 'N/A'
    address_button = soup.find('button', attrs={'aria-label': lambda x: x and x.startswith('Äá»‹a chá»‰:')})
    if address_button:
        address_div = address_button.find('div', class_='Io6YTe')
        address = address_div.get_text(strip=True) if address_div else 'N/A'

    # Láº¥y sá»‘ Ä‘iá»‡n thoáº¡i
    phone = 'N/A'
    phone_button = soup.find('button', attrs={'aria-label': lambda x: x and x.startswith('Sá»‘ Ä‘iá»‡n thoáº¡i:')})
    if phone_button:
        phone_div = phone_button.find('div', class_='Io6YTe')
        phone = phone_div.get_text(strip=True) if phone_div else 'N/A'

    # Láº¥y thumbnail
    thumbnail = 'N/A'
    img_tag = soup.find('button', class_='aoRNLd')
    if img_tag:
        img = img_tag.find('img')
        if img:
            thumbnail = img.get('src', 'N/A')

    driver.quit()

    return {
        "address_detail": address,
        "phone": phone,
        "thumbnail": thumbnail
    }
def crawl_detail(link):
    driver = webdriver.Chrome()
    driver.get(link)
    time.sleep(6)  # Ä‘á»§ thá»i gian load

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # Äá»‹a chá»‰ chi tiáº¿t
    address = 'N/A'
    address_button = soup.find('button', attrs={'aria-label': lambda x: x and x.startswith('Address:')})
    if address_button:
        address_div = address_button.find('div', class_='Io6YTe')
        address = address_div.get_text(strip=True) if address_div else 'N/A'

    # Sá»‘ Ä‘iá»‡n thoáº¡i
    phone = 'N/A'
    phone_button = soup.find('button', attrs={'aria-label': lambda x: x and x.startswith('Phone:')})
    if phone_button:
        phone_div = phone_button.find('div', class_='Io6YTe')
        phone = phone_div.get_text(strip=True) if phone_div else 'N/A'

    # Website
    website = 'N/A'
    website_link = soup.find('a', attrs={'aria-label': lambda x: x and x.startswith('Website:')})
    if website_link:
        website = website_link.get('href', 'N/A')

    # Plus code
    plus_code = 'N/A'
    plus_button = soup.find('button', attrs={'aria-label': lambda x: x and x.startswith('Plus code:')})
    if plus_button:
        plus_div = plus_button.find('div', class_='Io6YTe')
        plus_code = plus_div.get_text(strip=True) if plus_div else 'N/A'

    # Price range
    price_range = 'N/A'
    price_div = soup.find('div', string=lambda x: x and 'per person' in x)
    if price_div:
        price_range = price_div.get_text(strip=True)

    # Open hours: table
    open_hours = {}
    try:
        table = soup.find('table', class_='eK4R0e')
        if table:
            for row in table.find_all('tr'):
                cols = row.find_all('td')
                if len(cols) >= 2:
                    day = cols[0].get_text(strip=True)
                    hours = cols[1].get_text(strip=True)
                    open_hours[day] = hours
    except Exception as e:
        print(f"âš ï¸ Lá»—i láº¥y giá» má»Ÿ cá»­a: {e}")

    driver.quit()

    return {
        "address_detail": address,
        "phone": phone,
        "website": website,
        "plus_code": plus_code,
        "price_range": price_range,
        "open_hours": open_hours
    }

if __name__ == "__main__":
    os.makedirs("data", exist_ok=True)
    while True:
        query = input("ğŸ” Nháº­p tá»« khoÃ¡ tÃ¬m kiáº¿m (0 Ä‘á»ƒ thoÃ¡t): ").strip()
        if query == "0":
            print("ğŸ‘‹ Káº¿t thÃºc!")
            break

        print(f"ğŸš€ Äang crawl danh sÃ¡ch quÃ¡n: {query}")
        items = crawl_list(query)

        # Tiáº¿p tá»¥c vÃ o tá»«ng link Ä‘á»ƒ crawl chi tiáº¿t
        for i, item in enumerate(items):
            if item["link"] != 'N/A':
                print(f"â¡ï¸ [{i+1}/{len(items)}] Láº¥y chi tiáº¿t: {item['name']}")
                detail = crawl_detail(item["link"])
                item.update(detail)
            else:
                print(f"âš ï¸ Bá» qua quÃ¡n khÃ´ng cÃ³ link: {item['name']}")

        # LÆ°u file
        filename = query.replace(' ', '_') + ".json"
        filepath = os.path.join("data", filename)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(items, f, ensure_ascii=False, indent=2)

        print(f"âœ… ÄÃ£ lÆ°u káº¿t quáº£ chi tiáº¿t vÃ o {filepath}")
